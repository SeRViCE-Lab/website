<!doctype html>
<html lang="en">
<!-- Mathjax script -->
	<head>
		<script type="text/x-mathjax-config">
				MathJax.Hub.Config({
					  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
				});
		</script>

		<script type="text/javascript" src="MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>

		<meta charset="utf-8">

		<title>A 3-DoF Neuro-Adaptive Patient Pose Correcting System for Frameless and Maskless Cancer Radiotherapy</title>

		<meta name="description" content="A case for automating head and neck cancer radiotherapy treatment">
		<meta name="author" content="Olalekan Ogunmolu">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/league.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/serif.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h4>A 3-DoF Neuro-Adaptive Patient Pose Correcting System for Frameless and Maskless Cancer Radiotherapy</h4>
					<p><font size="5" color="yellow">Towards Precise Patient Positioning during Head and Neck Cancer Radiotherapy</font></p>
					<p>
						<small><a href="http://ecs.utdallas.edu/~olalekan.ogunmolu">Olalekan Ogunmolu</a> / <a href="http://twitter.com/patmeansnoble">@patmeansnoble</a></small>
					</p>
				</section>

				<!-- Example of nested vertical slides -->
				<section data-transition="slide" data-background="#4d7e65" data-background-transition="zoom">
					<section data-background="images/HNCancerRegions.png" style="display:block;background:#000;opacity:0.75;">
						<h2>Background</h2>
						<p>Head and neck (H&N) cancers are among the most fatal of major cancers in the United States</p>					
						<br>
						<p>2014: 35% of all pharynx and oral cavity cancers developed led to fatility [Siegal, R. et. al]</p>
						<br>
						<p>Cancer kills almost 600,000 people each year in the U.S. alone. <br><a href="See more at: https://news.developer.nvidia.com/university-of-torontos-gpu-accelerated-cancer-research-wins-nvidia-foundation-award/#sthash.1asn8CaE.g665SP03.dpuf"><small><br>Source: NVIDIA Foundation Award.</small></a></p>
						<a href="#" class="navigate-down">
							<img width="78" height="138" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow" style="display:block;background:#000;opacity:0;">
						</a>
						<br>
					</section>	

					<section data-background="images/UTSW2017/igrt_setup.jpg" style="display:block;background:#000;opacity:0.75;">
						<h2>Background</h2>
						<p><font color="#DAF7A6" face="verdana" size="25">Radiation-based Treatment Procedures</font></p>					
						<a href="#" class="navigate-down">
							<img width="50" height="50" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow" style="background:#333;opacity:1;" align="middle">
						</a>
						<table border="5", width="80%">
							<thead>
								<tr>
									<th><font color="#DAF7A6"><center>IMRT</center></font></th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>
										<small><li type="square">Modulates dosage/shape of radiation beam <u>to precise size of tumor cells</li></small>
										<small><li type="square">Improves accuracy of <u>carefully</u> targeted radiation </li></small>
										<!-- <small><li type="square">Deviations in delivered irradiation dosage possible</li></small> -->
									</td>
								</tr>
								<th><font color="#DAF7A6"><center>IGRT</center></font></th>
								<tr>									
									<td>							
										<small><li type="square">Uncertainty in dose measures to malignant tissues necessitated use of IGRT</li></small>
										<small><li type="square">Goal: Assure precise localization of beam to target tumor cell. </li></small>
									</td>
								</tr>
							</tbody>
						</table>
						<aside class="notes">
							<small><li type="square">Good procedure for distributed cancer cells</li></small>
							<small><li type="square">Palliative treatment when tumors elimination is impossible</li></small>
							<small><li type="square">Shrinks cancer tumors pre-surgery  or tumor leftovers post-surgery</li></small>
							<li>
								While conventional RT uses rigid
								immobilization techniques such as masks, frames, arm po-
								sitioning devices or vacuum mattresses [1], IGRT methods
								employ ultrasound, 3D imaging systems, 2D X-ray devices
								and/or computed tomography to instantly amend positioning
								errors, and to improve daily radiotherapy fractions’ precision.

							</li>
						</aside>
					</section>	
				</section>

				<section>					
					<section data-background="#212F3C" >	
						<h3>The case for accurate patient positioning</h3>
						<ul>
							<li>Modern irradiation procedures allow dose errors of a few millimeters
								<ul>									
									<li>This informs computer calculation of highly precise radiation treatment plans on static planning images</li>
								</ul>
							</li>							
							<li>But static CT scans are unrealistic during real-time patient treatment
								<ul>
									<li>How to minimize/eliminate <u>interfractional variations</u> in treatment between different sessions?</li>
								</ul>
								</li>
						</ul>
						<aside class="notes" data-markdown>
							**Various interfractional gradients**
							- patient poisitioning, 

							- organ mobility, and 

							- anatomic variations during treatment. 
						</aside>
					</section>

					<section data-background="images/IGRT.png" data-background="#212F3C">
						<h2><small>Related Works</small></h2>
						<br>
						<a href="#/2" class="navigate-down">
							<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow" >
						</a>
					</section>
					<section data-background="#212F3C">	
							<h3>What it entails</h3>
							<p>Accurate markers are placed inside a patient's body after consultation with a medic</p>
							<br>
							<p>Few days afterwards, a radiation-based scan (CT) of the markers is performed to localize the exact position of the markers in the gland</p>
							<br>
							<p>The scan provides the size and shape of the cancer cells for computerized treatment planning calculations</p>
						<!-- <br> -->
						<a href="#/2" class="navigate-down">
							<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow" >
						</a>
					</section>
					<section data-background="#212F3C">	
							<p>Current IGRT radiation-based systems include <br><a href="http://ac.els-cdn.com/S0360301613002137/1-s2.0-S0360301613002137-main.pdf?_tid=dd1ca758-9953-11e5-9bd0-00000aab0f6c&acdnat=1449102297_262e84a8d76bc146866d3936a3390fce"><small>[Jennifer De Los Santos et.al., 2012]</small></a></p>
							<br>
							<ul>
								<li>	
										Electronic Portal imaging detectors <br><small>e.g. IGRT and MV imaging; 1 - 2 mm accuracy; does not acquire 3D volumetric info</small>
								</li>
								<br>
								<li>	
										Cone-beam CT <small>retractable conventional x-ray tube and amorphous silicon x-ray detectors mounted either orthogonal to the treatment beam axis; used in lung/throat/liver, brain, head and neck cancer</small>
										<!-- <p><small></small></p> -->
								</li>
								<br>
								<li>	
										Fan-beam CT <small>in-room gantry-moving CT linac system to move across the patient instead of couch moving patient into the scanner as in conventional CT designs</small>
								</li>
							</ul>
							<a href="#/2" class="navigate-down">
							<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow" >
						</a>
					</section>
					<section data-background="#212F3C">	
							<ul>
								<li>	
										Stereoscopic imaging <br><small>used in CyberKnife; 2D imaging system; accuracy < 1mm</small>
								</li>
								<br>	
								<li>	
										Combination alignment systems: optical imaging and 2-D kV orthogonal imaging<br>
										<small>	Facilitates localization of rigid and mobile targets which may be volumetrically aligned with CBCT</small>
								</li>							
							</ul>
							<br>
							<a href="#/2" class="navigate-right">
							<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Right arrow" style="transform: rotate(-90deg); -webkit-transform: rotate(-90deg);">
						</a>
					</section>
				</section>

				<section data-markdown data-background="#212F3C" data-background-transition="zoom">
					###Research Motivation

					-	Clinical studies show that small perturbations cause high sensitivity to IMRT treatment dose <font color="#8A2BE2">[L. Xing, 2000.]</font>

					-	6D couch motion compensaion system is not time-optimal in treatments

							- <small>Treatment is often stopped for a medical physicist to recalibrate patient set-up when there is a deviation from target pose</small>

					- 	Evidence of treatment discomfort and severe pain from long hours of minimally invasive surgery <font color="#8A2BE2">[Takakura, T., et al., 2010]</font>
						<br>
						<a href="#/2" class="navigate-right">
						<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Right arrow" style="transform: rotate(-90deg); -webkit-transform: rotate(-90deg);">
					</a>
				</section>

				<section>
				<section data-background="#212F3C" data-background-transition="zoom">
					<h2>Related Work</h2>
					<ul>
							<li>
								Frameless and Maskless Cranial SRS <font color="purple">[Cervino et. al. 2010]</font>
							</li>
							<br>
							<li>	
								Idea was to verify accuracy of IGRT systems without rigid frames on face 
								<!-- <br><small><p>Frameless and maskless stereoscopic radiosurgery</p></small> -->
							</li>
							<br>
							<li>	
								Employed deformable masks of the following sort:
							</li>	
							<br>
					</ul>								
									<img src="images/frame2.png" width="250", height = "200"/>
									<img src="images/varian.png" width="250", height = "200" />
									<img src="images/cbct.png" width="250", height = "200"/>
									<!-- style="transform: rotate(-90deg); -webkit-transform: rotate(-90deg);" -->
				</section>

				<section id="fragments" data-background-color="#212F3C">
					<h3>Pros</h3>
						<ul>	
							<li>
								<p class="fragment fade-out" >
								<small>Anthropomorphic head phantoms employed in checking the accuracy of a 3D surface imaging system (AlignRT Vision System)</small>
								</p> 
							</li>
							<br>
							<li>
								<p class="fragment fade-out">
								<small>Compared results from an infra-red optical tracking system with the AlignRT vision software system</small>
								</p>
							</li>
							<br>
							<li>	
								<p class="fragment fade-out">
								<small>For different couch angles, the difference between phantom positions recorded by the two systems were within 1mm 		displacement  and 1° rotation</small>
								</p>
							</li>
							<br>
							<li>
								<p class="fragment fade-out">
								<small>Patient motion due to couch motion was less than 0.2mm</small>
								</p>
							</li>
								<!-- <br> -->
								<a href="#/2" class="navigate-down">
								<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
							</a>
						</ul>				
				</section>
					<section id="fragments" data-background-color="#212F3C">
						<h3>Cons</h3>
							<ul>	
								<li>
									<p class="fragment fade-out" >
									<small>6DOF positioning systems model the human body rigidly</small>
									</p> 
								</li>
								<br>
								<li>
									<p class="fragment fade-out">
									<small>No accounting for flexibility/curvature of the neck</small>
									</p>
								</li>
								<br>
								<li>	
									<p class="fragment fade-out">
									<small>Limited positioning of patient can reduce effectiveness</small>
									</p>
								</li>
								<br>
								<li>
									<p class="fragment fade-out">
									<small>Patient motion due to couch motion was less than 0.2mm</small>
									</p>
								</li>
								<br>
								<li>
									<p class="fragment fade-out">
								<small>If patient moves, therapy must be stopped, patient repositioned, costs time and money</small>
								</p>
								</li>
									<br>
									<a href="#/2" class="navigate-right">
									<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Right 	arrow" style="transform: rotate(-90deg); -webkit-transform: rotate(-90deg);">
								</a>
							</ul>				
					</section>
				</section>

				<section data-markdown data-background-color = "#212F3C">
					####Aims
					-	Accurate and automatic patient positioning system (pre-treatment)

					-	In-treatment automatic and accurate patient positioning with patient motion compensation


					####Objectives
					-	Surface-image control of the cranial flexion/extension motion of a patient during simulated H&N RT (pre-treatment)

					-	<small>Use radiation-transparent soft robot system for positioning/manipulation tasks</small>

						
						<br>
						<a href="#/2" class="navigate-right">
						<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Right arrow" style="transform: rotate(-90deg); -webkit-transform: rotate(-90deg);">
					</a>
	
				</section>

			<section>

				<section data-background = "#212F3C">	
						<h3><a font-color = "blue">Overview</a></h3>
						<p><small>Demonstrate optimal control of the 1-DOF intra-cranial control of patient motion during H&N Cancer RT</small></p>
						<p><small>Testbed consists of a Mannequin head with a neck/torso motion simulator lying in a supine position on an inflatable air bladder (IAB)</small></p>
						<p><small>System includes two two-port SMC Pnematics Co. proportional valves, to actuate bladder</small></p>
						<p><small>Two different Kinect RGB-D cameras measure the patient's position in real-time</small></p>
						<p><small>Work builds upon our earlier results from the initial iunvestigation</small></p>
						<br>
							<a href="#/2" class="navigate-right">
							<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down 	arrow" style="transform: rotate(-0deg); -webkit-transform: rotate(0deg);">
							</a>
				</section>

				<section data-background="#212F3C">
					<h3>System Set-up</h3>
					<p>
						<img src="images/setup.png" />
					</p>
					<a href="#" class="navigate-down">
						<img width="178" height="238" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>

				<section data-background="#212F3C">
					<h3>System Set-up</h3>
					<p>
						<img src="images/setup2.png" />
					</p>
					<a href="#" class="navigate-down">
						<img width="178" height="238" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>
			</section>

			<section>  <!-- Vision Description -->				
				<section data-markdown data-background="#212F3C" data-background-transition="zoom">	
					###Vision-based Head Mannequin Position Estimation: An Overview

					- Two Kinect RGB-D Cameras employed for position-based visual servoing.		
					
					- One is a time-of-flight sensor (Kinect2) while the other one is based on the structured light principle (Kinect1).
					
					- As with all electronic range perception instruments, noise is a nuisance.

					- We filter out the noise from each sensor using a linear Kalman filter at a local site and do a track-to-track fusion of the estimates at central site.
					<a href="#" class="navigate-down">
						<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>

				<section data-background="#212F3C">
					<h3>Depth Map from the two Kinect Sensors</h3>
					<p>
						<img src="images/raw-kinects.png" />
					</p>
					<a href="#" class="navigate-down">
						<img width="140" height="160" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>

				<section data-markdown data-background="#212F3C">
					###Drawbacks from the kinect1 sensor
					-	Kinect1 has a higher noise covariance that affects the accuracy of pose measurements

					-	Some materials such as the human hair and shiny surfaces do not reflect infrared wavelengths properly.

					-	There is an offset baseline between the emitter and camera because the sensor operates on the principle of stereo-matching. This contributes to measurement errors. 

					-	Occluding contours of objects are not properly outlined and this can cause a non-distinction between foreground and background.

					[Reference: Shotton, J., et. al. (2013). Real-Time Human Pose Recognition in Parts from Single Depth Images. CoMMuNiCatioNs of tHe aCM, 56(1)](http://doi.org/10.1145/2398356.2398381)
					<a href="#" class="navigate-down">
						<img width="140" height="160" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>

				<section data-markdown data-background="#212F3C">
					###Kinect 2 sensor
					-	Uses an active infra-red filter which reduces dependence on ambient lighting/other consumer device.

					-	Uses temporal and spatial averaging in the neighborhood of <i>N</i> pixels to improve the range resolution.

					-	Aliaising effects are minimized from the unambiguous distance range. 

					-	Uses multiple exposure settings to minimize saturation from highly reflective materials.

					[Reference: Gokturk, S. B., Yalcin, H., & Bamji, C. (2004). A Time-Of-Flight Depth Sensor - System Description, Issues and Solutions.](http://doi.org/10.1109/CVPR.2004.17)

					<a href="#" class="navigate-down">
						<img width="140" height="160" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>

				<section data-markdown data-background="#212F3C">
					-	Despite the improved performance of the v2, noise remains an issue, as is the case for every electronic perception system. 

					- 	To alleviate this, we employ a multisensor data fusion to cancel jitter from both Kinect sensors' observations. 

					-	Use local Kalman Filter estimates of each sensor's observations.

					-	Then fuse the local KF estimates at a central site.

					<a href="#" class="navigate-down">
						<img width="140" height="160" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>
			</section>

			<!-- Kalman Filter Models -->
			<section>				
				<section data-background="#212F3C">
					<h3>
					<a font color = "blue">Face Detection</a></h3>

					<p><li>Calibrate the kinect 1 camera's frame to the kinect 2's frame of reference.</li> </p>

					<p> <li>Use OpenCV Haar Cascade Classifiers to detect pupils of each eye of the mannequin.</li> </p>

					<p> <li>Perform detection on an NVIDIA K1100M GPU to speed up detection performance.</li></p>

					<a href="#" class="navigate-down">
						<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>					
				</section>

				<section data-background="#212F3C">
					<h3>
						<a font color = "blue">Local Kalman filters</a>
					</h3>
					<ul>
						<li>
							<b>Problem</b>: Find the state estimates $\hat{\textbf{x}}(i)$ that minimize the mean-squared  error to the true sensors' state $\textbf{x}(i)$, given the observation sequence, $z(1), \cdots, z(j)$
						</li>
						<li> that is, 
							<p><small>
							\begin{align} \label{eq.est_expt}

							\hat{\textbf{x}}(i|j)&= \text{arg } \min_{\hat{\textbf{x}}(i|j)\in \mathbb{R}^n}
							\mathbb{E}\{(\textbf{x}(i) - \hat{\textbf{x}})(\textbf{x}(i) - \hat{\textbf{x}})|z(1), \cdots, z(j)\} 	\nonumber \\
							&\triangleq\mathbb{E}\{\textbf{x}(i)|{z}(1), \cdots, {z}(j)\} \triangleq\mathbb{E}\{\textbf{x}(i)|{Z}^j\}

							\end{align}		
							</small></p>					
						</li>
						<li>					
							The covariance of the estimation error is  
							<p><small>
								\begin{align}
								\textbf{P}(i|j)\triangleq \mathbb{E}\{(\textbf{x}(i) - \hat{\textbf{x}}(i|j)(\textbf{x}(i) - \hat{\textbf{x}}(i|j)^T | Z^j\}.
								\end{align}
							</small></p>
						</li>
					</ul>
					<a href="#" class="navigate-down">
						<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>
				<section data-background="#212F3C">
					<h3>
						<a font color = "blue">Sensors Model</a>
					</h3>

					<h5><a font color="cyan">Assumptions</a></h5> 
					<ol>
					<li type="disc">Model of state transition matrix is common to both sensors</li>

					<li type="disc">The process noise is unknown</li>
					</ol>

					<h5><a font color="cyan">Modeling procedure</a></h5> 
					<ul>						
						<li>
							Denote $\textbf{x}(k)=[d(k),\,\dot{d}(k)]^T \in \mathbb{R}^2$ as the state vector of interest; where $d(k)$ represents the distance from the sensor's origin to the head of the mannequin
						</li>
						<li>
							And let $\Delta T $ be the time between steps $k-1$ and $k$.
						</li>
					</ul>
					<a href="#" class="navigate-down">
						<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>

				<section data-background="#212F3C">
					<h3>
						<a font color="cyan">Modeling procedure</a>					
					</h3>
					<ul>
						<li>
							The model state update equations are thus 
								\begin{equation}
								\textbf{x}_k = \textbf{F}_k\textbf{x}_{k-1}+\textbf{B}_k\textbf{u}_k+\textbf{G}_k\textbf{w}_k
								\label{eq:state_model}
								\end{equation}
								<p><small>where 
								$\textbf{F}(k) \in \mathbb{R}^{2\times 2}$ is the state transition matrix given by
					
							\begin{equation}
							\textbf{F} = \begin{bmatrix}
							1 & \Delta T \\
							0	& 1
							\end{bmatrix} 
							\end{equation}</small></p>
							<p><small>$\textbf{u}(k) \in \mathbb{R}^2$ is the control input,</small></p>
							<p><small>$\textbf{B}(k)$ is the control input matrix that maps inputs to system states,</small></p>
							<p><small>$\textbf{G}(k) \in \mathbb{R}^{ 2 \times 2}$ is the process noise matrix, and
							$\textbf{w}(k) \in \mathbb{R}^2$ is a random variable that models the state uncertainty.</small>
							</p>
						</li>
					</ul>
					<a href="#" class="navigate-down">
						<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>			
				<section data-background="#212F3C">
					<h3>
						<a font color="blue"> Modeling Procedure</a>						
					</h3>
					<ul>
						<li>	
							In our model, $u(k)$ = 0. Therefore the model evaluates to
							<p>
								\begin{align}\label{eq.accelmodel}
								\textbf{x}_k =  \textbf{F}_k \textbf{x}_{k-1}+ \textbf{G}_k \textbf{w}_k
								\end{align}
							</p>
						</li>
						<p><small>
							where $ \textbf{w}_k$ is the effect of an unknown input causing an acceleration $a_k$ in the head position and $ \textbf{G}_k$ applies that effect to the state vector, $ \textbf{x}_k$
						</small></p>
						<li>						
						<p>
							We set $\textbf{G}_k$ to identity and set $\textbf{w}(k) \sim \mathcal{N}(0, \textbf{Q}(k))$,
						</p>
						</li>
						<li>
							<p>And set the covariance matrix $\textbf{Q}(k)$ set to a random walk sequence defined by
									$\textbf{W}_k={[\frac{{\Delta T}^2}{2}, \Delta T ]}^T$ </p>
						</li>
						<li>Therefore, <small>
							\begin{align}
							\textbf{Q} &= \textbf{W}\textbf{W}^T{\sigma_a}^2
							= \begin{bmatrix}
							\dfrac{{\Delta T}^4}{4} &	\dfrac{{\Delta T}^3}{2} \\
							\dfrac{{\Delta T}^3}{2} & {\Delta T}^2
							\end{bmatrix}{\sigma_a}^2.
							\end{align}</small>
						</li>
					</ul>
					<a href="#" class="navigate-down">
						<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>

				<section data-background="#212F3C">
					<h3>
						<a font color="blue"> Modeling Procedure</a>						
					</h3>
					<ul>
						<li>	
							We transform kinect1's observations, $z_1(k)$, into kinect2's observation, $z_2(k)$, using the relation
							<p>
								\begin{equation}\label{eq:sensors_obs}
								{z}_s= \textbf{H}_s(k)\textbf{x}(k)+{v}_s(k) \qquad \qquad s = 1,2
								\end{equation}
							</p>
						</li>
						<p><small>
								where $\textbf{H}_s(k) ={\begin{bmatrix}  1 & 0 \end{bmatrix} }^T$ maps the system's state space into the observed space, and ${v}_s(k) \in \mathbb{R}$ is a random variable that models the sensors' error. We define ${v}_s(k)$ as a normally distributed random variable with zero mean and variance $\sigma_{rs}^2$.
						</small></p>
						<p><small>
								The random sequences $v_s(k)$ are assumed to be independednt and uncorrelated in time.
						</small></p>
						<li>						
						<p>
							The linear Kalman filter prediction and update phases are computed at every time step based on
						</p>
						</li>
					</ul>
					<a href="#" class="navigate-down">
						<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>
				<section data-background="#212F3C">
					<h3>
						<a font color="blue"> Modeling Procedure</a>						
					</h3>
					<TABLE BORDER="5"    WIDTH="100%"   CELLPADDING="1" CELLSPACING="2">

					    <TR>
					      <TH COLSPAN="4"><BR><H4 ALIGN="Center" font color="cyan">Kalman Filter Equations</H4>
					      </TH>
					    </TR>

					    <TR>
					      <TH ALIGN="Center"><small>Prediction Phase</small></TH>
					      <TH ALIGN="Center"><small>Update Phase</small></TH>
					    </TR>

					    <TR ALIGN="CENTER">
					      <TD>							      	
					      	<p><small>
					      		\begin{align}\label{eq:predict}
					      		\hat{\textbf{x}}_{k|k-1}&=\textbf{F}\hat{\textbf{x}}_{k-1|k-1} + \textbf{B}_k\textbf{u}_k  \nonumber \\ 
					      		\textbf{P}_{k|k-1}&=\textbf{F}_k\textbf{P}_{k-1|k-1}{\textbf{F}_k}^T + \textbf{Q}_k
					      		\end{align}
					      	</small></p>
					      </TD>

					      <TD>				      	
					      	<p><small>
					      	\begin{align} \label{eq:update}
					      	\textbf{K}_k &=  \textbf{P}_{k|k-1}{ \textbf{H}_k}^T{[ \textbf{H}_k \textbf{P}_{k|k-1}{ \textbf{H}_k}^T+ \textbf{R}_k]}^{-1}
					      	\nonumber \\ 
					      	\hat{ \textbf{x}}_{k|k}&=\hat{ \textbf{x}}_{k|k-1} +  \textbf{K}_k ( \textbf{z}_k -  \textbf{H}_k \hat{ \textbf{x}}_{k|k-1}) %\tilde{y}_k  
					      	\nonumber \\ 
					      	\textbf{P}_{k|k}&=( \textbf{I} -  \textbf{K}_k \textbf{H}_k) \textbf{P}_{k|k-1}
					      	\end{align}
					      	</small></p>
					      </TD>   
					    </TR>
					</TABLE>
					<p><small>
						where $\hat{ \textbf{x}}_{k|k-1}$ and $ \textbf{P}_{k|k-1}$ are the state prediction vector and the prediction covariance matrix respectively, and
					</small></p>

					<p><small>
						$ \textbf{K}_k$, $\hat{ \textbf{x}}_{k|k}$, and $ \textbf{P}_{k|k}$ are respectively the KF gain, posteriori state estimate and its state covariance matrix.
					</small></p>

					<p><small>
						Through exploration process model kinematics, and exploitation of the physics both sensors, we found the following values to successfully model the variance of the process noise and local KF signal noise:

					</small></p>
					<a href="#" class="navigate-down">
						<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>

				<section data-background="#212F3C">
					<h3>
						<a font color="blue"> Modeling Procedure</a>						
					</h3>
					<TABLE BORDER="5"    WIDTH="100%"   CELLPADDING="1" CELLSPACING="2">

					    <TR>
					      <TH COLSPAN="4"><BR><H4 ALIGN="Center" font color="cyan">Variance of local KFs</H4>
					      </TH>
					    </TR>

					    <TR>
					      <TH ALIGN="Center"><small>Kinect1</small></TH>
					      <TH ALIGN="Center"><small>Kinect2</small></TH>
					    </TR>

					    <TR ALIGN="CENTER">
					      <TD>							      	
					      	<p><small>
					      	\begin{align} 
					      		{\sigma_{r1}}^2 &= 70mm^2 \\					      		
					      		\sigma_a 		&= 2000 mm^2; 
					      	\end{align}
					      	</small></p>
					      </TD>

					      <TD>				      	
					      	<p><small>
					      	\begin{align}
					      		{\sigma_{r2}}^2 &= 60mm^2 \\
					      		\sigma_a 		&= 2000 mm^2; 
					      	\end{align}
					      	</small></p>
					      </TD>   
					    </TR>
					</TABLE>
				<a href="#" class="navigate-down">
					<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
				</a>
				</section>

				<section data-background="#212F3C">
					<div class="fig figcenter fighighlight"> 
					  <img src="images/CASE2016/KFXbox.png" width="45%" height="450", border="0" style="float:left;">
					  <img src="images/CASE2016/KFKinect2.png" width="50%" height="450"  border="0" style="float:right;">  
					  <div class="figcaption" align="left"><a  font color="blue">Results from filtering the observations from each sensor</a>
					  </div>
					</div>
					<!-- <img src="images/CASE2016/KFXbox.png" width="75%" height="35%">
					<img src="images/CASE2016/KFKinect2.png" width="75%" height="85%"> -->
					<a href="#" class="navigate-down">
						<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>
				<section data-background="#212F3C">
					<h3 ><a font color = "blue">Local KF Filtering Results</a></h3>
					<ul>
						<li>
							The steady-state performance of both sensors include a reduction in the variance of the observation sequence from Kinect 1 by $80.81\%$, while the Kinect 2 shows an improvement in noise rejection by almost $60\%$ . 
						</li>
					</ul>
					<a href="#" class="navigate-down">
						<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>

				<section data-background="#212F3C">
					<h3>
						<a font color="blue">Data Fusion</a>
					</h3>
					<ul>
						<li>
							The estimates (predictions) obtained in the foregoing are passed through Linux FIFO pipes to a central site for fusion
						</li>
						<li>
							At the central site, the predictions $\hat{\textbf{x}}_{F}(k|k)$ are combined according to 
						<p>
							\begin{align}
								\hat{\textbf{x}}_{F}(k|k) &= \textbf{P}_{F}(k|k)\sum\limits_{i=1}^{N}\left[{\textbf{P}_s}^{-1}(k|k)\hat{\textbf{x}}_s(k|k)\right] \nonumber \\
								\text{where } \textbf{P}_{F}(k|k) &= \left[\sum\limits_{i=1}^{N} {\textbf{P}_s}^{-1}(k|k)\right]^{-1}.
								\end{align}	
						</p>
						</li>
						<li>
							<small>
								Note: We assume a state model common to both sensors and adopt a variance-weighted average of each local track in the global track fusion algorithm
							</small>
						</li>
					</ul>
					<a href="#" class="navigate-down">
						<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>
				
				<section data-background="#212F3C">
					<h3><a font color="blue">Fusion Results</a></h3>
					<img src="images/CASE2016/fusion2.png">
					<a href="#" class="navigate-down">
						<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>
				
				<section data-background="#212F3C">
					<h3><a font color="blue">Fusion Take-aways.</a></h3>
					<ul>
						<li>
							 The fusion of the local tracks produces better estimates, with improved signal to noise ratio. 
						</li>
						<li>
							The fused estimate assigns more weight to the less noisy signal from Kinect 2. 
						</li>
						<li>
							We demonstrate improved accuracy of the effective signal to be used in our control algorithm to no more than a standard deviation of 0.75mm from the true position of an object. 				
						</li>
						<li>
							The noise spikes in the fused tracks when the process state estimates are yet to converge can be attributed to the noisy initialization of pixels in the sensors  before they attain their steady state values. 
						</li>
					</ul>
					<a href="#" class="navigate-down">
						<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>

			</section data-background="#212F3C">
				
			<section data-background="#212F3C">
				<section data-background="#212F3C">
					<h3><a font color="blue">Identification and Control</a></h3>
					<ul>
						<li>Collect lagged input-output data $Z^N = \{u(1) \cdots u(N) \quad y(1) \cdots y(N)\}$ offline; then determine <i>best model</i> from a set of candidate model sets</li>

						<li>where <i>$y_i$</i> is the fused track estimate and <i>$u_i$</i> = current to the pneumatic valve</li>

						<li>Let the model structure be a differentiable mapping from a connected, compact subset $\mathcal{D}_{\mathfrak{M}}$ of $\mathcal{R}^d$ to a model set $\mathfrak{M}^*$, such that the gradients of the predictor functions are stable</li>

						<li>Therefore, we can model external disturbances/stochastic variables as additive white noise sequence ...
						</li>
					</ul>
					<a href="#" class="navigate-down">
						<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>

				<section data-background="#212F3C">
					<h3><a font color="blue">Identification procedure</a></h3>
					<ul>
						<li>such that we obtain a stochastic state space sequence of the form:</li>
						<small><li>
							\begin{align} \label{eq:sysid_stochss}
							\mathbf{x}(k+1) = \mathbf{A x} (k) + \mathbf{B u}(k) + \mathbf{w}(k) \nonumber \\
							\mathbf{y}(k) = \mathbf{Cx}(k) + \mathbf{Du}(k) + \mathbf{v}(k)
							\end{align}
						</li></small>
						<li>$\mathbf{w}(k) \text{ and } \mathbf{v}(k)$ compensate for system disturbances and model uncertainties</li>

						<li>We estimate the states  $\mathbf{x}(k)$ and from measurable units $u$ and $y$ such that we end up with a linear regression problem</li>

						<aside class="notes">
							where all the unknown matrix entries are linear combinations of the measured inputs and output variables. 
						</aside>

						<li>
							This can be written as
							\begin{align} \label{eq:sys_idstate_minimal}
							Y(k) = \Theta \Phi(k) + E(k)
							\end{align}
						</li>
					</ul>
					<a href="#" class="navigate-down">
						<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>

				<section data-background="#212F3C">
					<ul>
							where 
							\[ Y(k) = \left[ \begin{array}{c}
							\mathbf{x}(k+1) \\
							\mathbf{y}(k)
							\end{array} \right],
							%
							\hspace{0.3em}
							%
							\Theta= \left[ \begin{array}{cc}
							\mathbf{A} & \mathbf{B} \\
							\mathbf{C} & \mathbf{D}
							\end{array}\right]
							\]
							
							\[\Phi(k) = \left[\begin{array}{c}
							\mathbf{x}(k) \\ \mathbf{u}(k)
							\end{array} \right] \hspace{0.2em} \text{ and  }
							%
							{E}(k) = \left[ \begin{array}{c}
							\mathbb{E}(w(k)) \\
							\mathbb{E}(v(k))
							\end{array}\right].
							\]
							<br>
						<li><b>Assumption:</b>
							Noise model is white $\rightarrow$ unbiased model
						</li>
						<br>

						<li>
							Therefore, estimate $\mathbf{A}, \mathbf{B}, \mathbf{C}, \text{ and } \mathbf{D}$ matrices by the linear least squares regression
						</li>
						<br>

						<li>
							Estimate $\mathbb{E}(\mathbf{w}(k))$ and  $\mathbb{E}(\mathbf{v}(k))$ as a sampled sum of squared errors of the residuals
						</li>
					</ul>
				</section>

				<section data-background="#212F3C">
					<h3>Parameter Estimation</h3>
					<ul>
						<li>	
							Xtize $u(k)$ and $y(k)$ as a linear difference equation of the form
						</li>
						<p>
							<small>
								\begin{eqnarray}\label{eq:lineardiff}
								y(k) &=-a_1y(k-1)-\cdots-a_{n_a}y(k-n_a) \nonumber \\
								&  -b_1u(k-1) - \cdots -b_{n_b}u(k-n_b) -e(k) \nonumber \\ &-c_1e(k-1) -c_{n_c}e(k-n_c)
								\end{eqnarray}
							</small>
						</p>

						<aside class="notes">
							$e(k)$ describes the equation error as a moving average of white noise, and we assume $e(k)$ has a bias-variance term $\lambda$.
						</aside>

						<li>
							The above equation can be translated to an ARMAX model of the form
						</li>

						<small><p>
							\begin{align}\label{eq:sysid_TF}
							\hat{y}(k) &= G(q, \theta)u(k) + H(q, \theta)\hat{e}(k) \\
							\text{with}  \quad  G(q, \theta) &= \dfrac{B(q)}{A(q)},\text{	 	 } H(q, \theta) = \dfrac{C(q)}{A(q)} \nonumber
							\end{align}
						</p></small>

						<li>
							$ G(q, \theta)$ represents the transfer function from input to output predictions, and $H(q, \theta)$ denotes the transfer function of prediction errors to the output model, $\hat{y}(k)$
						</li>

						<aside class="notes">
							$q$ is the z-transform, $z^{-1}$
						</aside>
					</ul>
				</section>

				<section>
					<ul>
						<li>
						$A(q)$, $B(q)$, and $C(q)$ are polynomials defined as 
						\begin{align}\label{eq:sysid_ABC}
						A(q) &= 1 + a_1 q^{-1} + \cdots + a_{n_a}q^{-n_a} , \nonumber \\
						B(q) &= b_1q^{-1}+ \cdots + b_{n_b}q^{n_b},  \nonumber  \\
						C(q) &= 1 + c_1 q^{-1} + \cdots + c_{n_c}q^{-n_c}
						\end{align}

						</li>

						<li>
							The estimation problem is to predict the estimates, $\hat{y}(k|\theta)$ so that the errors, $\varepsilon(t,\theta) = \parallel y(t) - \hat{y}(t|\theta) \parallel_p$ are minimized by the choice of an appropriate p-norm criterion function
						</li>

					</ul>
				</section>
			</section>
			
			<section>
				<h2>Export to PDF</h2>
				<p>Presentations can be <a href="https://github.com/hakimel/reveal.js#pdf-export">exported to PDF</a>, here's an example:</p>
				<iframe src="https://www.slideshare.net/slideshow/embed_code/42840540" width="445" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:3px solid #666; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>
			</section>			

			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// // Full list of configuration options available at:
			// // https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				// Display the page number of the current slide
				slideNumber: false,

				// Push each slide change to the browser history
				history: true,

				// Enable keyboard shortcuts for navigation
				keyboard: true,
				// Turns fragments on and off globally
				fragments: true,
				// Enable the slide overview mode
				overview: true,
				// Enables touch navigation on devices with touch input
				touch: true,
				// Flags if speaker notes should be visible to all viewers
				showNotes: false,
				// Flags if speaker notes should be visible to all viewers
				// showNotes: false,
				// Number of milliseconds between automatically proceeding to the
				// next slide, disabled when set to 0, this value can be overwritten
				// by using a data-autoslide attribute on your slides
				 autoSlide: 0,	
				// Stop auto-sliding after user input
				autoSlideStoppable: true,
				// Opens links in an iframe preview overlay
    			previewLinks: true,
				// Enable slide navigation via mouse wheel
				// mouseWheel: true
				// // Hides the address bar on mobile devices
				 hideAddressBar: true,
				// Parallax background image
				parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

				// Parallax background size
				parallaxBackgroundSize: '', // CSS syntax, e.g. "2100px 900px"

				// Number of pixels to move the parallax background per slide
				// - Calculated automatically unless specified
				// - Set to 0 to disable movement along an axis
				parallaxBackgroundHorizontal: null,
				parallaxBackgroundVertical: null,
				math: {
				    mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
				    config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
				},

				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Transition speed
				transitionSpeed: 'default', // default/fast/slow

				// Transition style for full page slide backgrounds
				backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom

				// Number of slides away from the current that are visible
				viewDistance: 3,

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },					
					// MathJax
					{ src: 'plugin/math/math.js', async: true }
				]
			});
		</script>

	</body>
</html>
